{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOBGuKjfybLz",
    "outputId": "8f041981-a8ad-4606-d68c-17bd7caaed55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p4yWnKIzmga",
    "outputId": "32f871c8-6cd3-4222-9d2c-cf0b89931269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Get:1 https://packages.microsoft.com/ubuntu/22.04/prod jammy InRelease [3,632 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Get:4 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main armhf Packages [15.5 kB]\n",
      "Get:5 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main all Packages [1,243 B]\n",
      "Get:6 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main arm64 Packages [40.0 kB]\n",
      "Get:7 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 Packages [172 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,172 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Hit:10 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,613 kB]\n",
      "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,223 kB]\n",
      "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.2 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,734 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,413 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\n",
      "Get:23 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,449 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,512 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [52.2 kB]\n",
      "Get:26 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,318 kB]\n",
      "Get:27 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,498 kB]\n",
      "Fetched 27.7 MB in 4s (6,436 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  odbcinst unixodbc\n",
      "The following NEW packages will be installed:\n",
      "  msodbcsql17 odbcinst unixodbc\n",
      "0 upgraded, 3 newly installed, 0 to remove and 57 not upgraded.\n",
      "Need to get 783 kB of archives.\n",
      "After this operation, 164 kB of additional disk space will be used.\n",
      "Get:1 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 msodbcsql17 amd64 17.10.6.1-1 [746 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 odbcinst amd64 2.3.9-5ubuntu0.1 [9,930 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n",
      "Fetched 783 kB in 1s (1,157 kB/s)\n",
      "Selecting previously unselected package odbcinst.\r\n",
      "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 123629 files and directories currently installed.)\r\n",
      "Preparing to unpack .../odbcinst_2.3.9-5ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking odbcinst (2.3.9-5ubuntu0.1) ...\r\n",
      "Selecting previously unselected package unixodbc.\r\n",
      "Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\r\n",
      "Selecting previously unselected package msodbcsql17.\r\n",
      "Preparing to unpack .../msodbcsql17_17.10.6.1-1_amd64.deb ...\r\n",
      "debconf: unable to initialize frontend: Dialog\r\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\n",
      "debconf: falling back to frontend: Readline\r\n",
      "Unpacking msodbcsql17 (17.10.6.1-1) ...\r\n",
      "Setting up odbcinst (2.3.9-5ubuntu0.1) ...\r\n",
      "Setting up unixodbc (2.3.9-5ubuntu0.1) ...\r\n",
      "Setting up msodbcsql17 (17.10.6.1-1) ...\r\n",
      "odbcinst: Driver installed. Usage count increased to 1. \r\n",
      "    Target directory is /etc\r\n",
      "Processing triggers for man-db (2.10.2-1) ...\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
      "\r100   983  100   983    0     0   7657      0 --:--:-- --:--:-- --:--:--  7679\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    88  100    88    0     0   1119      0 --:--:-- --:--:-- --:--:--  1128\n",
      "W: https://packages.microsoft.com/ubuntu/22.04/prod/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
    "\n",
    "curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
    "\n",
    "sudo apt-get update\n",
    "\n",
    "sudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDdRvG0yzIo4",
    "outputId": "4522a6a6-6802-4e19-d38b-0ea801c425f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyodbc in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "!pip install pyodbc\n",
    "import pyodbc\n",
    "import holidays\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oR4LO4PUzQSY"
   },
   "outputs": [],
   "source": [
    "#Informações para conectar-se ao banco\n",
    "\n",
    "SERVER   = 'hostnoa.database.windows.net'\n",
    "DATABASE = 'dbnoa'\n",
    "USERNAME = 'template'\n",
    "PASSWORD = 'template'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NCNulQeHzUOW"
   },
   "outputs": [],
   "source": [
    "#Estabelecendo conexão\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};UID={USERNAME};PWD={PASSWORD};ENCRYPT={\"yes\"}'\n",
    "\n",
    "conn = pyodbc.connect(connectionString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sraW_pEkzaCz"
   },
   "outputs": [],
   "source": [
    "#Requisitando informações da API\n",
    "\n",
    "# link do open_weather: https://openweathermap.org/\n",
    "# https://openweathermap.org/api/one-call-3\n",
    "\n",
    "API_KEY = \"5643ad2c732eb521a8e7cd2723c3369c\"\n",
    "cidade = \"são vicente\"\n",
    "link = f\"https://api.openweathermap.org/data/2.5/weather?q={cidade}&appid={API_KEY}&lang=pt_br\"\n",
    "\n",
    "requisitions = requests.get(link)\n",
    "informations = requisitions.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zDCG-7yzhS5"
   },
   "outputs": [],
   "source": [
    "#Traduzindo o dia da semana\n",
    "\n",
    "def get_weekday(time_info):\n",
    "  #currentTime = datetime.now().strftime(\"%H:%M:%S\")\n",
    "  weekDay = datetime.date.today().weekday()\n",
    "\n",
    "  day = [\"Segunda-feira\", \"Terça-feira\", \"Quarta-feira\", \"Quinta-feira\", \"Sexta-feira\", \"Sabado\", \"Domingo\"]\n",
    "\n",
    "  return day[weekDay]\n",
    "\n",
    "#Traduzindo o mês\n",
    "\n",
    "def get_month(time_info):\n",
    "\n",
    "  month = [\"Janeiro\", \"Fevereiro\", \"Março\", \"Abril\", \"Maio\", \"Junho\", \"Julho\", \"Agosto\", \"Setembro\", \"Outubro\", \"Novembro\", \"Dezembro\"]\n",
    "\n",
    "  return month[time_info.month]\n",
    "\n",
    "# Pegando a data (chave primária)\n",
    "\n",
    "def get_date(time_info):\n",
    "  return str(time_info.year) + '-' + str(time_info.month) + '-' + str(time_info.day)\n",
    "\n",
    "#Hoje é feriado?\n",
    "\n",
    "def get_holiday(time_info):\n",
    "  feriado = holidays.country_holidays(\"BR\", subdiv=\"SP\")\n",
    "\n",
    "  feriados_ano = feriado[(str(time_info.year) + '-' + \"1\" + '-' \"1\"): (str(time_info.year) + '-' + \"12\" + '-' \"31\")]\n",
    "  data = str(time_info.year) + '-' + str(time_info.month) + '-' + str(time_info.day)\n",
    "  for feriado in feriados_ano:\n",
    "    if(data == feriado):\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gs9sasLoqPKW"
   },
   "outputs": [],
   "source": [
    "#montando as camadas\n",
    "class ResidualLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, change_filters=False, SE=False, k=4, **kwargs):\n",
    "        super(ResidualLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.summ_activation = tf.keras.layers.Activation('swish')\n",
    "\n",
    "        self.main_layers = [tf.keras.layers.GroupNormalization(groups=32),\n",
    "                            tf.keras.layers.Activation('swish'),\n",
    "                            tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same', use_bias=False),\n",
    "                            tf.keras.layers.Conv2D(filters*4, kernel_size=3, padding='same', use_bias=False, strides=strides),\n",
    "                            tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same', use_bias=False),\n",
    "                            tf.keras.layers.GroupNormalization(groups=32),\n",
    "                            tf.keras.layers.Activation('swish'),\n",
    "                            tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same', use_bias=False),\n",
    "                            tf.keras.layers.Conv2D(filters*4, kernel_size=3, padding='same', use_bias=False),\n",
    "                            tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same', use_bias=False),\n",
    "                            tf.keras.layers.Activation('swish')]\n",
    "\n",
    "        if SE:\n",
    "            self.queeze_block = [tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.Flatten(),\n",
    "                                 tf.keras.layers.Dense(filters//k),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Activation('swish'),\n",
    "                                 tf.keras.layers.Dense(filters, activation='sigmoid'),\n",
    "                                 tf.keras.layers.Reshape([1, 1, filters])]\n",
    "            self.SE = True\n",
    "        else:\n",
    "            self.SE = False\n",
    "\n",
    "        if change_filters or strides != 1:\n",
    "            self.secundary_layers = [tf.keras.layers.GroupNormalization(groups=32),\n",
    "                                     tf.keras.layers.Activation('swish'),\n",
    "                                     tf.keras.layers.Conv2D(filters, kernel_size=1, padding='same', use_bias=False, strides=strides)]\n",
    "        else:\n",
    "             self.secundary_layers = None\n",
    "\n",
    "    def call(self, x):\n",
    "        data = tf.identity(x)\n",
    "\n",
    "        for layer in self.main_layers:\n",
    "            data = layer(data)\n",
    "\n",
    "        if self.SE:\n",
    "            excitation_factor = tf.identity(data)\n",
    "            for squeeze_layer in self.queeze_block:\n",
    "                excitation_factor = squeeze_layer(excitation_factor)\n",
    "\n",
    "            data = data*excitation_factor\n",
    "\n",
    "        if self.secundary_layers is not None:\n",
    "            for layer in self.secundary_layers:\n",
    "                x = layer(x)\n",
    "\n",
    "        return self.summ_activation(x+data)\n",
    "\n",
    "#montando o modelo personalizado\n",
    "class BinaryClassifier(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BinaryClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.init_convolution = [tf.keras.layers.RandomRotation((0.5), value_range=(0, 1)),\n",
    "                                 tf.keras.layers.RandomBrightness((0.5), value_range=(0,1)),\n",
    "                                 tf.keras.layers.Conv2D(filters=64, kernel_size=6, strides=2, padding='same', activation=tf.keras.layers.Activation('swish')), #128x128x3 -> 64x64x64\n",
    "                                 tf.keras.layers.BatchNormalization()]\n",
    "\n",
    "        self.residual_layers = [ResidualLayer(filters=128, strides=2, change_filters=True),   #64x64x64  ->  32x32x256\n",
    "                                ResidualLayer(filters=128, strides=2, change_filters=False),\n",
    "                                ResidualLayer(filters=256, strides=2, change_filters=True),\n",
    "                                ResidualLayer(filters=256, strides=2, change_filters=False)]   #8x8x512    ->  4x4x512\n",
    "\n",
    "        self.classifier_layers = [tf.keras.layers.GlobalAveragePooling2D(), #4x4x512=8192\n",
    "                                  tf.keras.layers.Dense(units=1, activation=\"sigmoid\")] #(final probabilities)\n",
    "\n",
    "    def call(self, x):\n",
    "        for init_conv in self.init_convolution:\n",
    "            x = init_conv(x)\n",
    "\n",
    "        for residual_layer in self.residual_layers:\n",
    "            x = residual_layer(x)\n",
    "\n",
    "        for classifier_layer in self.classifier_layers:\n",
    "            x = classifier_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiClassifier(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.init_convolution = [tf.keras.layers.Conv2D(filters=128, kernel_size=6, strides=2, padding='same', activation=tf.keras.layers.Activation('swish')), #128x128x3 -> 64x64x64\n",
    "                                 tf.keras.layers.BatchNormalization()]\n",
    "\n",
    "        self.residual_layers = [ResidualLayer(filters=256, strides=2, change_filters=True),   #64x64x64  ->  32x32x256\n",
    "                                ResidualLayer(filters=256, strides=2, change_filters=False),\n",
    "                                ResidualLayer(filters=512, strides=2, change_filters=True),\n",
    "                                ResidualLayer(filters=512, strides=2, change_filters=False)]   #8x8x512    ->  4x4x512\n",
    "\n",
    "        self.classifier_layers = [tf.keras.layers.GlobalAveragePooling2D(), #4x4x512=8192\n",
    "                                  tf.keras.layers.Dense(units=3, activation=\"softmax\")] #(final probabilities)\n",
    "\n",
    "    def call(self, x):\n",
    "        for init_conv in self.init_convolution:\n",
    "            x = init_conv(x)\n",
    "\n",
    "        for residual_layer in self.residual_layers:\n",
    "            x = residual_layer(x)\n",
    "\n",
    "        for classifier_layer in self.classifier_layers:\n",
    "            x = classifier_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#carregando a arquitetura treinada\n",
    "custom_objects = {'ResidualLayer': ResidualLayer, 'BinaryClassifier': BinaryClassifier}\n",
    "binary_predictor = tf.keras.models.load_model('/content/drive/MyDrive/NOA/computer_vision_model/binary_classifier.keras', custom_objects=custom_objects)\n",
    "\n",
    "custom_objects = {'ResidualLayer': ResidualLayer, 'MultiClassifier': MultiClassifier}\n",
    "multi_predictor = tf.keras.models.load_model(\"/content/drive/MyDrive/NOA/computer_vision_model/multi_classifier.keras\", custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWLmOlKilpjW"
   },
   "outputs": [],
   "source": [
    "def prediction(image):\n",
    "    image = cv2.resize(image, (128, 128))/255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    binary_prediction = float(binary_predictor.predict(image, verbose=0)[0][0])\n",
    "    if binary_prediction > 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        multi_prediction = multi_predictor.predict(image, verbose=0)[0]\n",
    "        return np.argmax(multi_prediction)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHn4W4-q1rIa"
   },
   "outputs": [],
   "source": [
    "def read_images(mp4_path):\n",
    "    preds = []\n",
    "    for video_file in os.listdir(\"/content/drive/MyDrive/NOA/original_videos/08 10 2024\"):\n",
    "        vidcap = cv2.VideoCapture(\"/content/drive/MyDrive/NOA/original_videos/08 10 2024/\" + video_file)\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            if count%1000 == 0:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                pred = prediction(image)\n",
    "                preds.append(pred)\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "\n",
    "    return sum(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngcMufqH8tZm"
   },
   "outputs": [],
   "source": [
    "preds = read_images('/content/drive/MyDrive/NOA/original_videos/08 10 2024') #automatizar a data aqui =D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdnSuATpOaE8",
    "outputId": "5eba4eb0-0c1d-4b35-83f5-249406c62672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1M6Z6GVJswwJ"
   },
   "outputs": [],
   "source": [
    "time_info = datetime.datetime.now()\n",
    "\n",
    "dados = {\n",
    "    \"qt_pessoas\": str(preds),\n",
    "    \"nm_dia_semana\": get_weekday(time_info),\n",
    "    \"nm_mes\": get_month(time_info),\n",
    "    \"nm_clima\": informations['weather'][0]['description'],\n",
    "    \"ic_feriado\": get_holiday(time_info),\n",
    "    \"vl_temperatura_media\": round((((round(informations['main']['temp_max']-273, 2)) + (round(informations['main']['temp_min']-273, 2)))/2),2),\n",
    "    \"vl_umidade\": str(informations['main']['humidity']),\n",
    "    \"vl_velocidade_ar\": str(round(informations['wind']['speed']*3.6, 2)),\n",
    "    \"data\": get_date(time_info)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0XIdy_0V4ZGd"
   },
   "outputs": [],
   "source": [
    "#Inserindo informações no banco\n",
    "\n",
    "data = []\n",
    "\n",
    "for chave,dado in dados.items():\n",
    "  data.append(dado)\n",
    "\n",
    "query = \"insert into dataset(qt_pessoas, nm_dia_semana, nm_mes, nm_clima, ic_feriado, vl_temperatura_media, vl_umidade, vl_velocidade_ar, data) values (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(query, data)\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
